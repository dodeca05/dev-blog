---
layout: single
title: RAG에 대해서
categories: coding
---

오늘은 RAG가 무엇이고 이것을 활용하는 방법에 대해서 알아볼겁니다.

그런데 제가 선호하는 공부방법은 되도록 깊게 파는 것 입니다. 따라서 최대한 오픈소스를 안쓰고 구현해보겠습니다

# RAG란?

**Retrieval-Augmented Generation (RAG)**은 대형 언어 모델(LLM)의 성능을 향상시키기 위해 정보 검색(Retrieval)과 생성(Generation)을 결합한 기술입니다.  
기존 언어 모델은 학습된 데이터에 기반하여 응답을 생성하지만, RAG는 외부 지식 소스로부터 관련 정보를 검색하여 이를 보강한 후 더욱 정확하고 풍부한 답변을 생성합니다.

---

# RAG의 주요 구성 요소

RAG는 세 가지 주요 구성 요소로 이루어져 있습니다:

## 1. Retrieval (검색)
- **설명:** 주어진 입력에 대해 관련된 문서를 검색합니다.
- **방법:**  
  - BM25, FAISS, Elasticsearch 등 검색 엔진 또는 벡터 데이터베이스 활용  
  - 임베딩 모델을 사용해 의미적으로 유사한 문서를 찾음

## 2. Augmentation (보강)
- **설명:** 검색된 문서를 기반으로 언어 모델의 입력을 보강합니다.
- **방법:**  
  - 원본 입력과 검색된 문서를 결합하여 보다 정확한 답변 생성

## 3. Generation (생성)
- **설명:** 보강된 입력을 바탕으로 LLM이 최종 응답을 생성합니다.
- **특징:**  
  - 사전 학습 데이터뿐 아니라 최신 정보도 반영 가능

---

# RAG의 장점

- **정보의 최신성 유지:**  
  정적 데이터셋 기반의 기존 LLM과 달리, 실시간 또는 최신 문서 검색을 통해 최신 정보를 반영할 수 있음.
  
- **정확성 향상:**  
  외부 정보를 활용하여 모델의 환각(hallucination) 문제를 줄이고 신뢰할 수 있는 응답 제공.
  
- **다양한 도메인 적용 가능:**  
  금융, 의료, 법률 등 특정 도메인에 특화된 문서를 검색해 도메인 특화된 응답 생성 가능.

---

# RAG의 활용 사례

- **QA 시스템:**  
  검색 기반 질의응답 시스템 구축
  
- **대화형 AI:**  
  최신 정보가 필요한 챗봇 및 가상 비서 개발
  
- **문서 요약:**  
  방대한 문서에서 필요한 정보를 검색 후 요약 생성
  
- **코드 및 기술 문서 검색:**  
  프로그래밍 문서나 API 문서를 검색하여 개발자 지원

---

# RAG 구현 방법

- **FAISS + LLM:**  
  벡터 데이터베이스인 FAISS를 사용해 빠르게 문서를 검색하고 LLM으로 응답 생성

- **Elasticsearch 활용:**  
  구조화된 데이터 검색과 LLM 연계를 통해 고성능 QA 시스템 구축

- **LangChain 적용:**  
  RAG 기반 애플리케이션을 쉽게 구축할 수 있도록 지원하는 프레임워크 활용

---

# RAG의 한계 및 해결 방안

- **검색 품질 문제:**  
  적절한 검색 알고리즘과 임베딩 모델 선택 필요

- **성능 문제:**  
  실시간 검색이 필요한 경우 응답 속도 저하 가능 (캐싱 및 인덱싱 최적화 필요)

- **LLM의 오류:**  
  검색된 정보를 정확히 반영하도록 프롬프트 엔지니어링 및 모델 튜닝 필요

---

# 사내 Wiki를 RAG로 활용하기

- **예시:**  
  간단한 챗봇을 만들어 사내 Wiki 정보를 검색 및 활용하는 방식

---

# Vector DB를 RAG로 활용하기

- **개념:**  
  텍스트를 벡터화한 후 벡터 간 유사도를 계산하여 문서를 검색하고 활용

---

# 예제코드

## 벡터 값을 이용한 텍스트 유사도 계산

RAG에서는 텍스트 유사도를 계산하여 적절한 문서를 검색하는 것이 핵심입니다.  
이를 위해 텍스트 데이터를 벡터화한 후, 벡터 간의 유사도를 비교합니다.

### 1. 자연어를 벡터로 변환하는 과정

텍스트를 벡터로 변환하기 위해 **토큰화(Tokenization)** 및 **임베딩(Embedding)** 과정을 거칩니다.  
초창기에는 LLM을 사용했으나, 최근에는 특화된 임베딩 모델을 사용합니다.  
- **LLM 특화 모델:** 텍스트 생성 및 자연어 이해  
- **텍스트 유사도 검출 특화 모델:** 문서 검색 및 유사도 비교 최적화

### 2. 벡터 DB에서 LLM 임베딩 모델을 사용할 수 있을까?

- **설명:**  
  LLM 임베딩 모델을 벡터 DB에서 사용할 수 있지만, 텍스트 유사도 전용 모델보다 정확도가 낮을 수 있음.  
  특정 애플리케이션에서는 큰 문제가 되지 않을 수 있음.

### 3. 벡터 유사도 측정 방법

- **코사인 유사도 (Cosine Similarity):**  
  두 벡터의 방향 유사도를 측정  
- **유클리디안 거리 (Euclidean Distance):**  
  벡터 간의 직선 거리(차이)를 계산  
- **점곱 (Dot Product):**  
  벡터 크기를 반영하여 유사도 측정

일반적으로 코사인 유사도가 가장 널리 사용됩니다.

---

## 자연어를 임베딩하여 벡터 확인해보기

예제에서는 `sentence-transformers` 라이브러리를 사용하여 한국어 임베딩 모델(`jhgan/ko-sroberta-multitask` 또는 다른 모델)을 활용합니다.

### 임베딩 예제 코드

```python
import numpy as np
from sentence_transformers import SentenceTransformer

texts = ["사과는 빨간색이다"]
# 한국어 임베딩 모델 예시
# embedding_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
embedding_model = SentenceTransformer('sentence-transformers/msmarco-bert-base-dot-v5')
embedding = embedding_model.encode(texts)
embedding = np.array(embedding, dtype=np.float32)

print("임베딩 생성 완료")
print(embedding.shape)
print(embedding)
```
# 임베딩된 벡터들로 텍스트 유사도 알아보기 (유클리드 방식)

## 유클리디안 거리 (Euclidean Distance)란?
유클리디안 거리는 두 점(벡터) 간의 직선 거리를 의미합니다.  
이를 활용하면 임베딩된 벡터들 간의 절대적인 거리를 계산하여 텍스트 유사도를 측정할 수 있습니다.

**공식:**

$$
d(A, B) = \sqrt{\sum (A_i - B_i)^2}
$$

## 유클리드 방식의 텍스트 유사도 검출 예제

```python
import numpy as np
from sentence_transformers import SentenceTransformer

texts = [
    "사과는 빨간색이다",
    "사과는 최근 많이 비싸졌다.",
    "사과",
    "태양계는 매우 크다.",
    "석유는 사실 무한하다.",
    "그녀는 나에게 사과했다.",
    "사과박스",
    "지구와 달의 공전주기는 정확히 일치한다",
    "사과에는 비타민이 풍부하다",
    "비타민이 풍부한 과일은?"
]

embedding_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
embedding = embedding_model.encode(texts)
embedding = np.array(embedding, dtype=np.float32)

print("임베딩 생성 완료")
print(embedding.shape)
print(embedding)

db = []
for x in range(len(texts)):
    for y in range(x+1, len(texts)):
        distance = (sum([(embedding[x][i]-embedding[y][i])**2 for i in range(len(embedding[0]))]))**0.5
        db.append([distance, texts[x], texts[y]])
db = sorted(db)
for dis, textA, textB in db:
    print(textA, f"<--{dis}-->", textB)

```
#임베딩된 벡터들로 텍스트 유사도 알아보기 (cos 유사도 방식)
##코사인 유사도란?
두 벡터의 내적과 각 벡터의 크기를 이용하여 두 벡터 간의 유사도를 측정합니다.
벡터의 방향이 0도이면 100% 일치, 90도이면 0% 일치로 판단할 수 있습니다.

## 코사인 유사도 방식의 텍스트 유사도 검출 예제

```python
import numpy as np
from sentence_transformers import SentenceTransformer

def cosine_similarity(vec1, vec2):
    # 두 벡터의 내적 계산: vec1 ⋅ vec2 = ∑(vec1[i] * vec2[i])
    dot_product = np.dot(vec1, vec2)
    # 벡터의 크기(노름) 계산: ||vec|| = sqrt(∑(vec[i]^2))
    norm_vec1 = np.linalg.norm(vec1)
    norm_vec2 = np.linalg.norm(vec2)
    if norm_vec1 == 0 or norm_vec2 == 0:
        return 0.0
    return dot_product / (norm_vec1 * norm_vec2)

texts = [
    "사과는 빨간색이다",
    "사과는 최근 많이 비싸졌다.",
    "사과",
    "태양계는 매우 크다.",
    "석유는 사실 무한하다.",
    "그녀는 나에게 사과했다.",
    "사과박스",
    "지구와 달의 공전주기는 정확히 일치한다",
    "사과에는 비타민이 풍부하다",
    "비타민이 풍부한 과일은?"
]

embedding_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
embedding = embedding_model.encode(texts)
embedding = np.array(embedding, dtype=np.float32)

print("임베딩 생성 완료")
print(embedding.shape)
print(embedding)
```

#간단한 RAG 검색
유사도가 높은 3개만 가져와 봅시다

```python
import numpy as np
from sentence_transformers import SentenceTransformer

def cosine_similarity(vec1, vec2):
    dot_product = np.dot(vec1, vec2)
    norm_vec1 = np.linalg.norm(vec1)
    norm_vec2 = np.linalg.norm(vec2)
    if norm_vec1 == 0 or norm_vec2 == 0:
        return 0.0
    return dot_product / (norm_vec1 * norm_vec2)

texts = [
    "사과는 빨간색이다",
    "사과는 최근 많이 비싸졌다.",
    "사과",
    "태양계는 매우 크다.",
    "석유는 사실 무한하다.",
    "그녀는 나에게 사과했다.",
    "사과박스",
    "지구와 달의 공전주기는 정확히 일치한다"
]

embedding_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
embedding = embedding_model.encode(texts)
embedding = np.array(embedding, dtype=np.float32)
vector_db = [[vector, text] for vector, text in zip(embedding, texts)]

key = input("검색어를 입력하세요 >")
embedding_key = embedding_model.encode([key])[0]
vector_db.sort(key=lambda x: cosine_similarity(x[0], embedding_key), reverse=True)

k = 3
for _, text in vector_db[:k]:
    print(text)


db = []
for x in range(len(texts)):
    for y in range(x+1, len(texts)):
        similarity = cosine_similarity(embedding[x], embedding[y])
        db.append([similarity, texts[x], texts[y]])
db = sorted(db, reverse=True)
for dis, textA, textB in db:
    print(textA, f"<--{dis}-->", textB)
```

지금까지 진행한 예제를 합쳐서 LLM에 활용하는 방법을 소개합니다.
OpenAI에서 api를 호출시 RAG용 api option을 제공해주는 것 같긴 한데 저는 llama를 주로 사용하므로 예제에서는 검색된 문서들을 LLM의 프롬프트에 포함하여 질문에 대한 답변을 생성합니다.

다음 정보와 질의가 있습니다:
```
<Context>
전기차 충전 인프라 확장이 빠르게 진행되면서, 전기차 이용자들의 편의성이 크게 향상되고 있다.
--------
정부는 전기차 보조금을 기존보다 20% 추가 삭감하겠다고 발표하며, 친환경 자동차 시장에 큰 변화를 예고했다.
--------
테슬라는 완전 자율주행 소프트웨어의 대규모 업그레이드를 발표하며, 자동차 산업의 혁신을 가속화하고 있다.
</Context>

<Question>
전기차를 지금 사면 좋을까요?
</Question>
```

#RAG + Ollama
rag를 ollama로 결합하여 오로지 로컬환경에서 동작하는 ai를 만들어 봅시다
```python
import requests
import json
import numpy as np
from sentence_transformers import SentenceTransformer

def cosine_similarity(vec1, vec2):
    dot_product = np.dot(vec1, vec2)
    norm_vec1 = np.linalg.norm(vec1)
    norm_vec2 = np.linalg.norm(vec2)
    if norm_vec1 == 0 or norm_vec2 == 0:
        return 0.0
    return dot_product / (norm_vec1 * norm_vec2)

texts = [
    "2025년 서울 아파트 가격이 3개월 연속 하락세를 보이며, 부동산 시장의 불안정성이 커지고 있다.",
    "한국은행은 2025년 2월 기준금리를 동결하기로 결정하며, 시장의 예상을 뛰어넘는 결정을 내렸다.",
    "정부는 전기차 보조금을 기존보다 20% 추가 삭감하겠다고 발표하며, 친환경 자동차 시장에 큰 변화를 예고했다.",
    "AI 기술이 급속도로 발전하면서 국내 IT업계에서는 대규모 채용이 진행될 예정이라는 전망이 나왔다.",
    "사우디아라비아의 감산 연장으로 인해 국제 유가가 배럴당 90달러를 돌파하며, 경제적 파장이 예상된다.",
    "미국 연준은 인플레이션 대응을 위해 추가적인 금리 인상을 검토 중이며, 이는 글로벌 금융시장에 큰 영향을 미칠 것으로 보인다.",
    "2025년 프로야구 개막전이 오는 3월 29일 개최될 것으로 확정되며, 팬들의 기대감을 높이고 있다.",
    "테슬라는 완전 자율주행 소프트웨어의 대규모 업그레이드를 발표하며, 자동차 산업의 혁신을 가속화하고 있다.",
    "삼성전자는 3나노 반도체 기술을 공개하며, 반도체 시장에서의 경쟁력을 강화하는 행보를 보였다.",
    "중국과 유럽 간 무역 갈등이 심화되면서, 한국 경제에도 상당한 영향을 미칠 것으로 예상되고 있다.",
    "기후 변화 대응을 위해 각국이 탄소 중립 목표를 수정하며, 친환경 정책이 더욱 강화될 것으로 보인다.",
    "국내 스타트업 생태계가 빠르게 성장하며, 벤처캐피털 투자 유치 규모가 역대 최고치를 기록했다.",
    "2025년 대한민국 총선이 다가오며, 각 정당은 유권자의 표심을 잡기 위한 정책 경쟁에 돌입했다.",
    "전 세계적으로 해킹 공격이 증가하며, 사이버 보안의 중요성이 어느 때보다도 강조되고 있다.",
    "2025년 국제 우주 탐사 프로젝트가 본격적으로 시작되며, 우주 산업이 새로운 도약을 준비하고 있다.",
    "유럽연합은 새로운 개인정보 보호 규제를 발표하며, 글로벌 IT 기업들의 대응이 요구되고 있다.",
    "전기차 충전 인프라 확장이 빠르게 진행되면서, 전기차 이용자들의 편의성이 크게 향상되고 있다.",
    "2025년 세계 경제 성장률이 둔화될 것이라는 전망이 나오며, 각국 정부의 경제 대응책이 주목받고 있다.",
    "최근 도입된 블록체인 기술이 금융권에서 빠르게 확산되며, 새로운 핀테크 서비스들이 등장하고 있다.",
    "국내 대형 백화점들이 온라인 쇼핑몰과의 경쟁 심화로 인해 새로운 유통 전략을 모색하고 있다.",
    "김재현은 지마켓에서 일하고 있다."
]

embedding_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
embedding = embedding_model.encode(texts)
embedding = np.array(embedding, dtype=np.float32)
vector_db = [[vector, text] for vector, text in zip(embedding, texts)]

key = "전기차를 지금 사면 좋을까요?"
embedding_key = embedding_model.encode([key])[0]
vector_db.sort(key=lambda x: cosine_similarity(x[0], embedding_key), reverse=True)

k = 3
top_k_contexts = [text for _, text in vector_db[:k]]

prompt = f"""
다음 정보와 질의가 있습니다:

<Context>
{"\n--------\n".join(top_k_contexts)}
</Context>

<Question>
{key}
</Question>
위의 정보를 기반으로 답변을 작성해주세요.
"""

print("작성된 프롬프트")
print(prompt)

def call_ollama_llm(prompt):
    url = "http://localhost:11434/api/chat"  # Ollama 엔드포인트 예시
    headers = {"Content-Type": "application/json"}
    payload = {
        "messages": [{"role": "user", "content": prompt}],
        "model": "llama3.1:8b",
        "stream": False
    }
    response = requests.post(url, headers=headers, data=json.dumps(payload))
    if response.status_code == 200:
        result = response.json()
        return result['message']['content']
    else:
        print("[Error] Ollama LLM 호출 실패:", response.text)
        return ""

answer = call_ollama_llm(prompt)
print("LLM 응답:", answer)
```

# 벡터 DB의 다른 용도

## 1. 캐싱 (Caching)

### 개념
캐싱은 동일하거나 매우 유사한 질문에 대해 매번 새로운 답변을 생성하지 않고, 기존에 생성된 응답을 재활용하는 방식입니다.  
- **프로세스:**  
  - 사용자가 입력한 텍스트를 임베딩하여 벡터를 생성  
  - 기존 질문 벡터들과의 유사도를 계산  
  - 유사도가 일정 기준(예: 0.85) 이상이면 기존에 저장된 응답을 반환  
- **효과:**  
  - 불필요한 모델 호출을 줄여 응답 속도를 향상시키고, 시스템 부하를 경감할 수 있습니다.

### 캐싱 예제 코드

```python
import numpy as np
from sentence_transformers import SentenceTransformer

def cosine_similarity(vec1, vec2):
    # 두 벡터의 내적 계산
    dot_product = np.dot(vec1, vec2)
    # 벡터의 크기(노름) 계산
    norm_vec1 = np.linalg.norm(vec1)
    norm_vec2 = np.linalg.norm(vec2)
    if norm_vec1 == 0 or norm_vec2 == 0:
        return 0.0
    return dot_product / (norm_vec1 * norm_vec2)

texts = [
    "사과는 무슨색 인가요?",
    "라면 맛있게 끓이는 방법을 알려주세요",
    "사과는 어떤색 인가요?",
    "사과의 색이 궁금합니다.",
    "맛있는 라면은 어떻게 만드나요?"
]
answers = [
    "빨간색 입니다.",
    "동생 라면 뺏어먹는게 제일 맛있습니다.",
    "붉은색 입니다.",
    "빨강입니다.",
    "정성을 넣어 끓이면 됩니다."
]

embedding_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
print("임베딩 생성 완료")

cache_db = []

for i in range(len(texts)):
    print("질문 : ", texts[i])
    question_vector = np.array(embedding_model.encode(texts[i]), dtype=np.float32)
    cache = None
    for vector, cache_answer in cache_db:
        similarity = cosine_similarity(question_vector, vector)
        if similarity > 0.85:
            cache = cache_answer
            break

    if cache is None:
        print("응답을 새로 생성합니다.")
        answer = answers[i]
        cache_db.append([question_vector, answer])
    else:
        print("비슷한 질문을 했었습니다. 기존에 했던 응답을 다시 반환합니다.")
        answer = cache
    print("응답 : ", answer)
```
## 2. 검열/필터링 (Censorship or Filtering)

### 개념
검열 또는 필터링은 특정 질문이나 단어가 금지어나 부적절한 내용에 해당하는 경우, 이를 사전에 차단하는 기법입니다.  
- **프로세스:**  
  - 관리자가 필터링 대상 키워드(예: "마약", "살인", "마약 제조", "생명을 죽이다")를 임베딩하여 벡터화합니다.
  - 사용자의 입력 텍스트를 임베딩하여 벡터로 변환합니다.
  - 입력 벡터와 필터링 대상 벡터 간의 유사도를 측정합니다.
  - 유사도가 기준(예: 0.75) 이상이면 해당 질문을 차단하거나, 추가 승인 절차를 진행합니다.

### 검열/필터링 예제 코드

```python
import numpy as np
from sentence_transformers import SentenceTransformer

def cosine_similarity(vec1, vec2):
    # 두 벡터의 내적 계산
    dot_product = np.dot(vec1, vec2)
    # 벡터의 크기(노름) 계산
    norm_vec1 = np.linalg.norm(vec1)
    norm_vec2 = np.linalg.norm(vec2)
    if norm_vec1 == 0 or norm_vec2 == 0:
        return 0.0
    return dot_product / (norm_vec1 * norm_vec2)

texts = [
    "사과는 무슨색 인가요?",
    "라면 맛있게 끓이는 방법을 알려주세요",
    "마약 만드는 방법 알려주세요",
    "사람을 죽이자",
    "이 음식 정말 죽이네"
]

bans = ["마약", "살인", "마약 제조", "생명을 죽이다"]

embedding_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
bans_embedding = embedding_model.encode(bans)
bans_embedding = np.array(bans_embedding, dtype=np.float32)

print("임베딩 생성 완료")

for text in texts:
    print("질문: ", text)
    question_vector = np.array(embedding_model.encode(text), dtype=np.float32)
    for ban_vector in bans_embedding:
        similarity = cosine_similarity(ban_vector, question_vector)
        if similarity >= 0.75:
            print("이 질문에 대답하지 않

```
